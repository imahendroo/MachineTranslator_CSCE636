{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CSCE 636 PROJECT 2 - ISHAAN MAHENDROO - UIN:327002775"
      ],
      "metadata": {
        "id": "HqsVDFHSIrN9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8IzpplCtvGu9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_input = pickle.load(open('/content/Train_input','rb'))\n",
        "train_output = pickle.load(open('/content/Train_output', 'rb'))"
      ],
      "metadata": {
        "id": "acsB5bGSwTh0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_tr = 0\n",
        "max_ind_tr = 0\n",
        "max_len_ts = 0\n",
        "max_ind_ts = 0\n",
        "avg_tr = 0\n",
        "avg_ts = 0\n",
        "for i in range(len(train_input)):\n",
        "  tr_word = train_input[i].split()\n",
        "  l_tr = len(tr_word)\n",
        "  avg_tr += l_tr\n",
        "  if l_tr > max_len_tr:\n",
        "    max_len_tr = l_tr\n",
        "    max_ind_tr = i\n",
        "  ts_word = train_output[i].split()\n",
        "  l_ts = len(ts_word)\n",
        "  avg_ts += l_ts\n",
        "  if l_ts > max_len_ts:\n",
        "    max_len_ts = l_ts\n",
        "    max_ind_ts = i\n",
        "\n",
        "print(max_len_tr)\n",
        "print(train_input[max_ind_tr])\n",
        "print(avg_tr / 112000)\n",
        "print(max_len_ts)\n",
        "print(train_output[max_ind_ts])\n",
        "print(avg_ts / 112000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBUClqY9wrUJ",
        "outputId": "c847dfae-2fff-4cb2-ed91-219d19a877c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "a h a f a f c d c e a e b d b d a g a d c f b d a e a g b d c g \n",
            "22.096375\n",
            "47\n",
            "c d c e a f d e b d a e g a f f h b d c f b d a d k l b d c g a g ed ee a e ef a g m eg a h i j eh \n",
            "32.1445625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seeing the average and max length of a sequence in each dataset to determine how long my sequence_length should be. Considering the \"sentence\" when there is no whitespace"
      ],
      "metadata": {
        "id": "2FuGeJ0B7Yay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_output = []\n",
        "for sequence in train_output:\n",
        "  sequence = \"[start] \" + sequence + \"[end]\"\n",
        "  test_output.append(sequence)\n"
      ],
      "metadata": {
        "id": "Mahi9PDiF9Cv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rpPSYxLbG6l5",
        "outputId": "80cab6bb-99d3-4e2c-fde0-7c157eccff71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[start] b d b d c d c f a h e f g b d a d h i a f d j [end]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update each output language sequence to include \"[start] \" and \" [end]\"."
      ],
      "metadata": {
        "id": "9urK4mz5HGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_vectorization = layers.TextVectorization(\n",
        "    output_mode= \"int\",\n",
        "    max_tokens = 1000,\n",
        "    output_sequence_length= 40,\n",
        "    standardize = \"lower_and_strip_punctuation\"\n",
        ")\n",
        "\n",
        "output_vectorization = layers.TextVectorization(\n",
        "    output_mode = \"int\",\n",
        "    max_tokens = 1000,\n",
        "    output_sequence_length= 40 + 1,\n",
        "    standardize = \"lower\"\n",
        ")\n",
        "\n",
        "source_vectorization.adapt(train_input)\n",
        "output_vectorization.adapt(test_output)"
      ],
      "metadata": {
        "id": "qIgdb-oDy5qu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I made my vocabulary size 1000, I assumed there could be 26 + (26*26) combinations if I account for pairs of letters and then each letter by itself, so I rounded to 1000 in case there were some other combinations I didn't see while looking at the data."
      ],
      "metadata": {
        "id": "98sgevvnxMCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorization(inputs, outputs):\n",
        "  input = source_vectorization(inputs)\n",
        "  output = output_vectorization(outputs)\n",
        "  return ({\"input_lang\": input, \"output_lang\": output[:,:-1],}, output[:,1:])\n",
        "\n",
        "def make_dataset(inputs, outputs):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "  dataset = dataset.batch(64)\n",
        "  dataset = dataset.map(vectorization, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  dataset = dataset.shuffle(112000).prefetch(tf.data.AUTOTUNE).cache()\n",
        "  return dataset\n",
        "\n",
        "data = make_dataset(train_input, test_output)\n",
        "data = data.shuffle(1750) #another shuffle just for more robustness\n",
        "val_size = 200 #since I batched the data, doing .take or .skip considers batches not individual rows\n",
        "val_data = data.take(val_size)\n",
        "train_data = data.skip(val_size)"
      ],
      "metadata": {
        "id": "R2Bw05bBJk6K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I create a tensorflow dataset by passing the function vectorization to the input and output datasets. It returns the output_lang twice because the target is the output lang but the output lang is also an input during the transformer decoder portion of the model. So I need output lang as an input and output"
      ],
      "metadata": {
        "id": "DSuqV1FRyqt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_data.cardinality().numpy()) #validation dataset is 64 * 200 batches = 12800 rows or a bit over 10% of the data\n",
        "train_data.cardinality().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4NRMDRBqk6N",
        "outputId": "ba1d6f27-d23e-4cd8-ddb9-d37ef068e624"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1550"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_data.take(1):\n",
        "    print(f\"input_lang shape: {inputs['input_lang'].shape}\")\n",
        "    print(f\"output_lang shape: {inputs['output_lang'].shape}\")\n",
        "    print(f\"targets shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8RxHoNjWYKK",
        "outputId": "dc43fc13-ad47-4773-b3ee-07a33bddc17e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_lang shape: (64, 40)\n",
            "output_lang shape: (64, 40)\n",
            "targets shape: (64, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the datasets are made where for both train and validation, each input is a batch with size 64 containing sequences where each word (letter/letters in this case) is represented by an int and each sequence is of length 40. The dataset has input_lang,output_lang without [end] for the training phase, output_lang without the [start] for the ground truth."
      ],
      "metadata": {
        "id": "MaedFJ4BnspR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(input_dim = input_dim, output_dim = output_dim)\n",
        "    self.position_embeddings = layers.Embedding(input_dim = sequence_length, output_dim = output_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start = 0, limit = length, delta = 1)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def compute_mask(self, inputs, mask  = None):\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\"sequence_length\": self.sequence_length, \"input_dim\": self.input_dim, \"output_dim\": self.output_dim,})\n",
        "    return config"
      ],
      "metadata": {
        "id": "9PLevVnEm6g-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I wanted to embed my tensors to hopefully extract the meaning of each sentence. I felt doing positional embedding would be the best since I am doing text. Even though it is a made up language I still think order matters.\n",
        "\n",
        "I used the author of the books method for positional embedding over the cosine method. I simply have 2 embedding layers, one for the original word and one for the position, then the length of the input sequence, the input_dim (vocab_size), and the output_dim. So both the word and positions get passed through a keras embedding layer seperately and then added together as the result.\n",
        "\n",
        "Also a compute mask function so we can ignore padding 0s in the inputs."
      ],
      "metadata": {
        "id": "C1Zw_TOZvGcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.attention = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
        "    self.dense = keras.Sequential([layers.Dense(dense_dim, activation = \"relu\"), layers.Dense(embed_dim),])\n",
        "    self.layernormal1 = layers.LayerNormalization()\n",
        "    self.layernormal2 = layers.LayerNormalization()\n",
        "\n",
        "  def call(self, inputs, mask = None):\n",
        "    if mask is not None:\n",
        "      mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "    attention_layer = self.attention(inputs, inputs, attention_mask = mask)\n",
        "    dense_input = self.layernormal1(attention_layer + inputs)\n",
        "    dense_layer = self.dense(dense_input)\n",
        "    dense_output = self.layernormal2(dense_input + dense_layer)\n",
        "    return dense_output\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\"embed_dim\": self.embed_dim, \"dense_dim\": self.dense_dim, \"num_heads\": self.num_heads,})\n",
        "    return config"
      ],
      "metadata": {
        "id": "kL-sEUM5u5JM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the transformerendcoder, I followed the textbooks general outline of the structure as well. Doing:\n",
        "\n",
        "multihead attention layer => residual_1(multihead_output + inputs) => dense_projection => resdiual_2(residual_1_output + dense_proj_output).\n",
        "\n",
        "So the init and call functions follow that outline. I needed the embed_dim, dense_dim, and num_heads then included the required 5 layers. Then the call function is just passing through the layers in the above order."
      ],
      "metadata": {
        "id": "SG_fOVsn1Bmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.attention1 = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
        "    self.attention2 = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
        "    self.dense = keras.Sequential([layers.Dense(dense_dim, activation = 'relu'), layers.Dense(embed_dim),])\n",
        "    self.layernormal1 = layers.LayerNormalization()\n",
        "    self.layernormal2 = layers.LayerNormalization()\n",
        "    self.layernormal3 = layers.LayerNormalization()\n",
        "\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def get_causal_attention_mask(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    mask = tf.cast(i >= j, dtype = \"int32\")\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1,1], dtype = tf.int32)], axis = 0)\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "  def call(self, inputs, encoder_outputs, mask = None):\n",
        "    causal_mask = self.get_causal_attention_mask(inputs)\n",
        "    if mask is not None:\n",
        "      padding_mask = tf.cast(mask[:,tf.newaxis,:], dtype = \"int32\")\n",
        "      padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "    else:\n",
        "      padding_mask = mask\n",
        "\n",
        "    attention_layer1 = self.attention1(query = inputs, value = inputs, key = inputs, attention_mask = causal_mask)\n",
        "    attention_layer1 = self.layernormal1(inputs + attention_layer1)\n",
        "    attention_layer2 = self.attention2(query = attention_layer1, value = encoder_outputs, key = encoder_outputs, attention_mask = padding_mask)\n",
        "    attention_layer2 = self.layernormal2(attention_layer1 + attention_layer2)\n",
        "    dense_layer = self.dense(attention_layer2)\n",
        "    dense_output = self.layernormal3(attention_layer2 + dense_layer)\n",
        "    return dense_output\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\"embed_dim\": self.embed_dim, \"dense_dim\": self.dense_dim, \"num_heads\": self.num_heads,})\n",
        "    return config"
      ],
      "metadata": {
        "id": "RWAzLkRLDFov"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder follows a pretty similar structure to the encoder however there are a few key differences. There are now 7 layers in the decoder, an additional multihead attention layer that takes in the encoder's outputs as its input and an additional layer normalization layer to account for this additional multihead attention layer.\n",
        "\n",
        "So in the init function simialrly I needed the embed_dim, dense_dim, and num_heads again then below that are the 7 layers I need being 2 multihead_attentions, 3 layer_normalizations, and 2 Dense layers.\n",
        "\n",
        "The main difference between this and the encoder at least code-wise is that I needed to account for causal padding. The transformerdecoder is \"order agnostic\" meaning unlike the RNN which looks at its inputs one at a time, the decoder looks at the whole target sequence at the same time. This would be cheating and I would have perfect train accuracy with terrible validation if I didn't include this because the decoder would be able to see the answer (the n+1th key would just be copied to location n in the output because it can see the next key already, it doesn't need to predict it will just map). So the get causal_attention_mask will mask the outputs we shouldn't know yet with a matrix of the form 1,0,0... (next row) 1,1,0,0,0,... (next row) 1,1,1,0,0,0,... and so on.\n",
        "\n",
        "The call function works pretty similarly too as it is just following the structure of the decoder from the textbook. The main difference again is that one of the attention layers must use the encoder's outputs as its input. This is clear in the attention_layer_2 where the key and value of this attention_layer are the encoder_outputs. The first attention layer is taking the output_lang as input, the second attention layer is taking the (encoded) input_lang as the input. So it follows as:\n",
        "\n",
        "multihead_attention1(output_lang) => layer_norm1(output_lang + multihead_attention_output) => multihead_attention2(encoded_input_lang) => layer_norm2(layer_norm_1_output + multihead_attention2(encoded_input_lang)) => dense_projection => layernorm3(dense_output + layernorm_2_output)."
      ],
      "metadata": {
        "id": "s99WWYSU2MQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "input_seq_len = 40\n",
        "output_seq_len = 40\n",
        "vocab_size = 1000\n",
        "\n",
        "encoder_inputs = keras.Input(shape = (None,), dtype = \"int64\", name = \"input_lang\")\n",
        "x = PositionalEmbedding(input_seq_len, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape = (None,), dtype = \"int64\", name = \"output_lang\")\n",
        "x = PositionalEmbedding(output_seq_len, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation = \"softmax\")(x)\n",
        "\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "transformer.compile(optimizer = \"Adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "aO1P5lwRKNcN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For my initial attempt, I kept the structure of the model the same as the textbook again, meaning I have a pretty small model. I have the encoder inputs get passed through their positional embedding layer then through the transformer encoder. Then the output_lang gets passed into the decoder_inputs and through its respective positional embedding layer. Then the transformerdecoder layer uses the outputs from the encoder and the positionaly embedded output_lang tensors. After that I included a dropout even though it was my first attempt because it seems like common practice to include a dropout layer for transformer models so I tried it. Had the model seemed like it was underfitting I could remove it in following attempts. Similarly for the first attempt I followed the same embed_dim, num_heads, and dense_dim as the textbook to get an initial baseline model performance then tune as I needed. This is also why I started with 1 TransformerEncoder and 1 TransformerDecoder layer, I could stack more of these layers together if necessary for a complex problem. However as seen below, the model performed very well on this dataset. I believe it was able to get such high accuracy becasue the dataset was designed to be perfectly translated. Had it been a real language the accuracy could be lower."
      ],
      "metadata": {
        "id": "YsTLZiXy7ZN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Eg4yLCaXws6",
        "outputId": "02f41bac-c7b0-4f91-9475-c5aa08f2eaf0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_lang (InputLayer)     [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " output_lang (InputLayer)    [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding (Posi  (None, None, 256)            266240    ['input_lang[0][0]']          \n",
            " tionalEmbedding)                                                                                 \n",
            "                                                                                                  \n",
            " positional_embedding_1 (Po  (None, None, 256)            266240    ['output_lang[0][0]']         \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n",
            " formerEncoder)                                                                                   \n",
            "                                                                                                  \n",
            " transformer_decoder (Trans  (None, None, 256)            5259520   ['positional_embedding_1[0][0]\n",
            " formerDecoder)                                                     ',                            \n",
            "                                                                     'transformer_encoder[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, None, 256)            0         ['transformer_decoder[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, None, 1000)           257000    ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9204456 (35.11 MB)\n",
            "Trainable params: 9204456 (35.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.fit(train_data, epochs = 10, validation_data = val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLiIfEKMU05m",
        "outputId": "b83fa2a9-8a91-48ea-b077-3cfd4b028c01"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1550/1550 [==============================] - 69s 34ms/step - loss: 0.3416 - accuracy: 0.8689 - val_loss: 0.0737 - val_accuracy: 0.9713\n",
            "Epoch 2/10\n",
            "1550/1550 [==============================] - 52s 34ms/step - loss: 0.0634 - accuracy: 0.9763 - val_loss: 0.0426 - val_accuracy: 0.9841\n",
            "Epoch 3/10\n",
            "1550/1550 [==============================] - 50s 32ms/step - loss: 0.0386 - accuracy: 0.9861 - val_loss: 0.0311 - val_accuracy: 0.9886\n",
            "Epoch 4/10\n",
            "1550/1550 [==============================] - 51s 33ms/step - loss: 0.0289 - accuracy: 0.9898 - val_loss: 0.0261 - val_accuracy: 0.9909\n",
            "Epoch 5/10\n",
            "1550/1550 [==============================] - 50s 32ms/step - loss: 0.0228 - accuracy: 0.9921 - val_loss: 0.0188 - val_accuracy: 0.9932\n",
            "Epoch 6/10\n",
            "1550/1550 [==============================] - 52s 33ms/step - loss: 0.0193 - accuracy: 0.9934 - val_loss: 0.0229 - val_accuracy: 0.9920\n",
            "Epoch 7/10\n",
            "1550/1550 [==============================] - 51s 33ms/step - loss: 0.0162 - accuracy: 0.9944 - val_loss: 0.0098 - val_accuracy: 0.9965\n",
            "Epoch 8/10\n",
            "1550/1550 [==============================] - 51s 33ms/step - loss: 0.0144 - accuracy: 0.9951 - val_loss: 0.0115 - val_accuracy: 0.9960\n",
            "Epoch 9/10\n",
            "1550/1550 [==============================] - 52s 34ms/step - loss: 0.0119 - accuracy: 0.9959 - val_loss: 0.0099 - val_accuracy: 0.9966\n",
            "Epoch 10/10\n",
            "1550/1550 [==============================] - 50s 33ms/step - loss: 0.0112 - accuracy: 0.9961 - val_loss: 0.0116 - val_accuracy: 0.9959\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b48dd537610>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.save('transformer.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk9K-OhdV7zb",
        "outputId": "59f87027-2ae0-41c5-c670-f1f62cf13453"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SIMPLE CODE FOR TESTING"
      ],
      "metadata": {
        "id": "3f899JUAI7YY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_vocab = output_vectorization.get_vocabulary()\n",
        "sparse_index = dict(zip(range(len(sparse_vocab)), sparse_vocab))\n",
        "max_sent = 40"
      ],
      "metadata": {
        "id": "VEiczFLSsSEM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sentence(input):\n",
        "  token_inputs = source_vectorization([input])\n",
        "  decoded_sentence = \"[start]\"\n",
        "  for i in range(max_sent):\n",
        "    token_outputs = output_vectorization([decoded_sentence])[:, :-1] # get rid of last word in the output \"[end]\"\n",
        "    prediction = transformer([token_inputs, token_outputs])\n",
        "    sample_index = np.argmax(prediction[0,i,:]) #predictions makes an array of all the words, so look at word i and choose the highest probability\n",
        "    sample = sparse_index[sample_index]\n",
        "\n",
        "    if sample == \"[end]\":\n",
        "      break\n",
        "\n",
        "    decoded_sentence += \" \" + sample\n",
        "\n",
        "  return decoded_sentence[len(\"[start] \"):] # remove the [start]"
      ],
      "metadata": {
        "id": "SsJ0sGU59rLN"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = train_input[0]\n",
        "print(sentence)\n",
        "print(decode_sentence(sentence))\n",
        "print(train_output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew6kkOWDBrzX",
        "outputId": "85d50861-6eed-481b-e6df-c5c568073f3f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a f b d a d a h b d c d c f b d \n",
            "b d b d c d c f a h e f g b d a d h i a f d j\n",
            "b d b d c d c f a h e f g b d a d h i a f d j \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "  i = np.random.randint(len(train_input))\n",
        "  sentence = train_input[i]\n",
        "  print(sentence)\n",
        "  print(decode_sentence(sentence))\n",
        "  print(train_output[i])\n",
        "  print('-' * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVZXe-WNBCvF",
        "outputId": "9b088161-e863-406c-b9a3-37d8b88094ee"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a f a f a d c f b d c e a g c d c g \n",
            "c f b d a d d e c e a f f g c d c g a g i j a f h k\n",
            "c f b d a d d e c e a f f g c d c g a g i j a f h k \n",
            "------------------------------\n",
            "a h b d a e a d a d a e b d b d a g b d c g a e c d \n",
            "b d b d a e e b d a d f g b d c g a g i j a d h k a e l c d a e ed a h d m ee\n",
            "b d b d a e e b d a d f g b d c g a g i j a d h k a e l c d a e ed a h d m ee \n",
            "------------------------------\n",
            "a f a g c e a g c g a d c d a e b d c f \n",
            "c e c g c d b d a e g a d f h a g e i a g d j c f a f k l\n",
            "c e c g c d b d a e g a d f h a g e i a g d j c f a f k l \n",
            "------------------------------\n",
            "a d a e a e a e a f c f b d a d b d c g \n",
            "c f b d a f d e a e f a e g a e h b d c g a d j k a d i l\n",
            "c f b d a f d e a e f a e g a e h b d c g a d j k a d i l \n",
            "------------------------------\n",
            "a e a g a f c e a h b d a d b d b d b d b d \n",
            "c e b d b d b d a d f g b d a h e h i a f d j b d a g k l a e m\n",
            "c e b d b d b d a d f g b d a h e h i a f d j b d a g k l a e m \n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the model gets the translations correct. However since I have no test dataset yet this could be inaccurate of course. Since I randomly shuffled twice I do not know which samples were in my train or validation dataset so I just selected 5 random ones from the whole dataset. In practice if this was all the data I had it could be better to split this dataset into train,validate,and test. However since the test set is coming later I only did train and validate. And I feel seeing that it performed very well on the validation set is a strong indication of how it will perform on the unseen test set."
      ],
      "metadata": {
        "id": "LmGwOKhYNrlS"
      }
    }
  ]
}